{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b931514c-9cb9-41f3-b953-c0113985d4f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T03:11:00.851929Z",
     "iopub.status.busy": "2023-04-28T03:11:00.851602Z",
     "iopub.status.idle": "2023-04-28T03:11:02.594959Z",
     "shell.execute_reply": "2023-04-28T03:11:02.593046Z",
     "shell.execute_reply.started": "2023-04-28T03:11:00.851899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both\n",
      "both\n",
      "both\n",
      "both\n",
      "2d\n",
      "both\n",
      "both\n",
      "both\n",
      "both\n",
      "both\n",
      "both\n",
      "both\n",
      "both\n",
      "both\n",
      "3d\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [2, 2, 1, 3, 3], expected input[1, 1, 19, 127, 128] to have 2 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 232\u001b[0m\n\u001b[1;32m    230\u001b[0m MNet \u001b[38;5;241m=\u001b[39m MNet(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, kn\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), ds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m19\u001b[39m, \u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m256\u001b[39m))\n\u001b[0;32m--> 232\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mMNet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mprint\u001b[39m([e\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m output])\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [1], line 187\u001b[0m, in \u001b[0;36mMNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    186\u001b[0m     down11 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown11(x)\n\u001b[0;32m--> 187\u001b[0m     down12 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown12\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdown11\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m     down13 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown13(down12[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    189\u001b[0m     down14 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown14(down13[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [1], line 96\u001b[0m, in \u001b[0;36mDownsample.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCB3d(x)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCB2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCB3d(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [1], line 36\u001b[0m, in \u001b[0;36mCB3d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 36\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [1], line 19\u001b[0m, in \u001b[0;36mCNA3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py:607\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 607\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py:602\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    592\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    593\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    601\u001b[0m     )\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [2, 2, 1, 3, 3], expected input[1, 1, 19, 127, 128] to have 2 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNA3D(torch.nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kSize,stride,padding=(1,1,1),bias=True,norm_args=None,activation_args=None):\n",
    "        super().__init__()\n",
    "        self.norm_args = norm_args\n",
    "        self.activation_args = activation_args\n",
    "        \n",
    "        self.conv = torch.nn.Conv3d(in_channels,out_channels,kernel_size=kSize,stride=stride,padding=padding,bias=bias)\n",
    "        \n",
    "        if norm_args is not None:\n",
    "            self.norm = torch.nn.InstanceNorm3d(out_channels, **norm_args)\n",
    "        \n",
    "        if activation_args is not None:\n",
    "            self.activation = torch.nn.LeakyReLU(**activation_args)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        if self.norm_args is not None:\n",
    "            x = self.norm(x)\n",
    "        if self.activation_args is not None:\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "    \n",
    "class CB3d(torch.nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kSize=(3,3),stride=(1,1),padding=(1,1,1),bias=True,norm_args:tuple=(None,None),activation_args:tuple=(None,None)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = CNA3D(in_channels,out_channels,kSize=kSize[0],stride=stride[0],padding=padding,bias=bias,norm_args=norm_args[0],activation_args=activation_args[0])\n",
    "        \n",
    "        self.conv2 = CNA3D(out_channels,in_channels,kSize=kSize[1],stride=stride[1],padding=padding,bias=bias,norm_args=norm_args[1],activation_args=activation_args[1])\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.conv2(x)\n",
    "        return x\n",
    "    \n",
    "class BasicNet(torch.nn.Module):\n",
    "    norm_kwargs={\"affine\":True}\n",
    "    activation_kwargs={\"negative_slope\":1e-2,\"inplace\":True}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BasicNet,self).__init__();\n",
    "\n",
    "def FMU_sub(x2d, x3d, mode=\"sub\"):  \n",
    "        return torch.abs(x2d-x3d) \n",
    "\n",
    "class Downsample(BasicNet):\n",
    "    def __init__(self,in_channels,out_channels,mode: tuple, downsample=True, min_z=8):\n",
    "        super().__init__()\n",
    "        self.mode_in,self.mode_out = mode \n",
    "        self.downsample = downsample \n",
    "        self.FMU = FMU_sub\n",
    "        norm_args = (self.norm_kwargs, self.norm_kwargs)\n",
    "        activation_args = (self.activation_kwargs, self.activation_kwargs)\n",
    "        self.CB2d = None\n",
    "        \n",
    "        print(self.mode_out)\n",
    "        \n",
    "        if self.mode_out == '2d' or self.mode_out =='both':\n",
    "            self.CB2d = CB3d(in_channels=in_channels,out_channels=out_channels,kSize=((1,3,3),(1,3,3)),stride=(1,1),padding=(0,1,1),\n",
    "                             norm_args=norm_args,activation_args=activation_args)\n",
    "            \n",
    "        if self.mode_out=='3d' or self.mode_out=='both':\n",
    "            self.CB3d = CB3d(in_channels=in_channels,out_channels=out_channels,kSize=(3,3),stride=(1,1),padding=(1,1,1),\n",
    "                             norm_args=norm_args,activation_args=activation_args)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        if self.downsample:\n",
    "            if self.mode_in=='both':\n",
    "                x2d,x3d = x\n",
    "                p2d = F.max_pool3d(x2d,kernel_size=(1,2,2),stride=(1,2,2))\n",
    "                if x3.shape[2] >= self.min_z:\n",
    "                    p3d=F.max_pool3d(x3d,kernel_size=(2,2,2),stride=(2,2,2))\n",
    "                else:\n",
    "                    p3d=F.max_pool3d(x3d,kernel_size=(1,2,2),stride=(1,2,2))\n",
    "                    \n",
    "                x = self.FMU(p2d,p3d)\n",
    "                \n",
    "            elif self.mode_in=='2d':\n",
    "                x = F.max_pool3d(x,kernel_size=(1,2,2),stride=(1,2,2))\n",
    "                \n",
    "            elif self.mode_in=='3d':\n",
    "                if x.shape[2] >= self.min_z:\n",
    "                    x=F.max_pool3d(x,kernel_size=(2,2,2),stride=(2,2,2))\n",
    "                else:\n",
    "                    x=F.max_pool3d(x,kernel_size=(1,2,2),stride=(2,2,2))\n",
    "                    \n",
    "        if self.mode_out=='2d':\n",
    "            return self.CB2d(x)\n",
    "        elif self.mode_out=='3d':\n",
    "            return self.CB3d(x)\n",
    "        else: \n",
    "            return self.CB2d(x), self.CB3d(x)\n",
    "                \n",
    "class Upsample(BasicNet):\n",
    "    def __init__(self, in_channels, out_channels, mode: tuple):\n",
    "        super().__init__()\n",
    "        self.mode_in,self.mode_out = mode \n",
    "        self.FMU = FMU_sub\n",
    "        norm_args = (self.norm_kwargs,self.norm_kwargs)\n",
    "        activation_args=(self.activation_kwargs,self.activation_kwargs)\n",
    "        \n",
    "        if self.mode_out=='2d' or self.mode_out=='both':\n",
    "            self.CB2d=CB3d(in_channels=in_channels,out_channels=out_channels,\n",
    "                           kSize=((1,3,3),(1,3,3)),stride=(1,1),padding=(0,1,1),\n",
    "                           norm_args=norm_args,activation_args=activation_args)\n",
    "            \n",
    "        if self.mode_out=='3d' or self.mode_out=='both':\n",
    "            self.CB3d=CB3d(in_channels=in_channels,out_channels=out_channels,\n",
    "                           kSize=(3,3),stride=(1,1),padding=(1,1,1),\n",
    "                           norm_args=norm_args,activation_args=activation_args)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x2d, xskip2d, x3d, xskip3d = x\n",
    "        \n",
    "        tarSize=xskip2d.shape[2:]\n",
    "        up2d=F.interpolate(x2d,size=tarSize,mode=\"trilinear\",align_corners=False)\n",
    "        up3d=F.interpolate(x3d,size=tarSize,mode=\"trilinear\",align_corners=False)\n",
    "        \n",
    "        cat=torch.cat([FMU_sub(xskip2d,xskip3d,self.FMU),FMU_sub(up2d,up3d,self.FMU)],dim=1)\n",
    "        \n",
    "        if self.mode_out=='2d':\n",
    "            return self.CB2d(cat)\n",
    "        elif self.mode_out=='3d':\n",
    "            return self.CB3d(cat)\n",
    "        else:\n",
    "            return self.CB2d(cat),self.CB3d(cat)\n",
    "                \n",
    "class MNet(BasicNet):\n",
    "    def __init__(self, in_channels, num_classes, kn=(32, 48, 64, 80, 96), ds=True ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: channels of input\n",
    "            num_classes: output classes\n",
    "            kn: the number of kernels\n",
    "            ds: deep supervision\n",
    "            FMU: type of feature merging unit\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        channel_factor = {'sum': 1, 'sub': 1, 'cat': 2}\n",
    "        fct = 1##= channel_factor[FMU]\n",
    "\n",
    "        self.down11 = Downsample(in_channels, kn[0], ('/', 'both'), downsample=False)\n",
    "        self.down12 = Downsample(kn[0], kn[1], ('2d', 'both'))\n",
    "        self.down13 = Downsample(kn[1], kn[2], ('2d', 'both'))\n",
    "        self.down14 = Downsample(kn[2], kn[3], ('2d', 'both'))\n",
    "        self.bottleneck1 = Downsample(kn[3], kn[4], ('2d', '2d'))\n",
    "        self.up11 = Upsample(fct * (kn[3] + kn[4]), kn[3], ('both', '2d'))\n",
    "        self.up12 = Upsample(fct * (kn[2] + kn[3]), kn[2], ('both', '2d'))\n",
    "        self.up13 = Upsample(fct * (kn[1] + kn[2]), kn[1], ('both', '2d'))\n",
    "        self.up14 = Upsample(fct * (kn[0] + kn[1]), kn[0], ('both', 'both'))\n",
    "\n",
    "        self.down21 = Downsample(kn[0], kn[1], ('3d', 'both'))\n",
    "        self.down22 = Downsample(fct * kn[1], kn[2], ('both', 'both'))\n",
    "        self.down23 = Downsample(fct * kn[2], kn[3], ('both', 'both'))\n",
    "        self.bottleneck2 = Downsample(fct * kn[3], kn[4], ('both', 'both'))\n",
    "        self.up21 = Upsample(fct * (kn[3] + kn[4]), kn[3], ('both', 'both'))\n",
    "        self.up22 = Upsample(fct * (kn[2] + kn[3]), kn[2], ('both', 'both'))\n",
    "        self.up23 = Upsample(fct * (kn[1] + kn[2]), kn[1], ('both', '3d'))\n",
    "\n",
    "        self.down31 = Downsample(kn[1], kn[2], ('3d', 'both'))\n",
    "        self.down32 = Downsample(fct * kn[2], kn[3], ('both', 'both'))\n",
    "        self.bottleneck3 = Downsample(fct * kn[3], kn[4], ('both', 'both'))\n",
    "        self.up31 = Upsample(fct * (kn[3] + kn[4]), kn[3], ('both', 'both'))\n",
    "        self.up32 = Upsample(fct * (kn[2] + kn[3]), kn[2], ('both', '3d'))\n",
    "\n",
    "        self.down41 = Downsample(kn[2], kn[3], ('3d', 'both'))\n",
    "        self.bottleneck4 = Downsample(fct * kn[3], kn[4], ('both', 'both'))\n",
    "        self.up41 = Upsample(fct * (kn[3] + kn[4]), kn[3], ('both', '3d'))\n",
    "\n",
    "        self.bottleneck5 = Downsample(kn[3], kn[4], ('3d', '3d'))\n",
    "\n",
    "\n",
    "        self.outputs = torch.nn.ModuleList(\n",
    "            [torch.nn.Conv3d(c, num_classes, kernel_size=(1, 1, 1), stride=1, padding=0, bias=False)\n",
    "             for c in [kn[0], kn[1], kn[1], kn[2], kn[2], kn[3], kn[3]]]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        down11 = self.down11(x)\n",
    "        down12 = self.down12(down11[0])\n",
    "        down13 = self.down13(down12[0])\n",
    "        down14 = self.down14(down13[0])\n",
    "        bottleNeck1 = self.bottleneck1(down14[0])\n",
    "\n",
    "        down21 = self.down21(down11[1])\n",
    "        down22 = self.down22([down21[0], down12[1]])\n",
    "        down23 = self.down23([down22[0], down13[1]])\n",
    "        bottleNeck2 = self.bottleneck2([down23[0], down14[1]])\n",
    "\n",
    "        down31 = self.down31(down21[1])\n",
    "        down32 = self.down32([down31[0], down22[1]])\n",
    "        bottleNeck3 = self.bottleneck3([down32[0], down23[1]])\n",
    "\n",
    "        down41 = self.down41(down31[1])\n",
    "        bottleNeck4 = self.bottleneck4([down41[0], down32[1]])\n",
    "\n",
    "        bottleNeck5 = self.bottleneck5(down41[1])\n",
    "\n",
    "        up41 = self.up41([bottleNeck4[0], down41[0], bottleNeck5, down41[1]])\n",
    "\n",
    "        up31 = self.up31([bottleNeck3[0], down32[0], bottleNeck4[1], down32[1]])\n",
    "        up32 = self.up32([up31[0], down31[0], up41, down31[1]])\n",
    "\n",
    "        up21 = self.up21([bottleNeck2[0], down23[0], bottleNeck3[1], down23[1]])\n",
    "        up22 = self.up22([up21[0], down22[0], up31[1], down22[1]])\n",
    "        up23 = self.up23([up22[0], down21[0], up32, down21[1]])\n",
    "        \n",
    "        up11 = self.up11([bottleNeck1, down14[0], bottleNeck2[1], down14[1]])\n",
    "        up12 = self.up12([up11, down13[0], up21[1], down13[1]])\n",
    "        up13 = self.up13([up12, down12[0], up22[1], down12[1]])\n",
    "        up14 = self.up14([up13, down11[0], up23, down11[1]])\n",
    "\n",
    "\n",
    "        if self.ds:\n",
    "            features = [up14[0]+up14[1], up23, up13, up32, up12, up41,up11]\n",
    "            return [self.outputs[i](features[i]) for i in range(7)]\n",
    "        else:\n",
    "            return self.outputs[0](up14[0]+up14[1])\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MNet = MNet(1, 3, kn=(2, 2, 2, 2, 2), ds=True)\n",
    "    input = torch.randn((1, 1, 19, 255,256))\n",
    "    output = MNet(input)\n",
    "\n",
    "    print([e.shape for e in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60ab062-b8e2-45da-9752-362c61c26695",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /datasets/brats20_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
