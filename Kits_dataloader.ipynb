{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3f9d95f-9cdb-41aa-bd75-b309b6a231af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nibabel\n",
      "  Downloading nibabel-5.1.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.9/dist-packages (from nibabel) (1.23.4)\n",
      "Requirement already satisfied: packaging>=17 in /usr/local/lib/python3.9/dist-packages (from nibabel) (23.0)\n",
      "Installing collected packages: nibabel\n",
      "Successfully installed nibabel-5.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torchmetrics\n",
      "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.12.1+cu116)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.4.0)\n",
      "Installing collected packages: torchmetrics\n",
      "Successfully installed torchmetrics-0.11.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nibabel\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6051c48b-0245-4a91-bf28-7d74d8603921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import glob\n",
    "import os\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imsave\n",
    "import torchmetrics\n",
    "from torchmetrics import Dice\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b72ee882-8b6c-46cf-8a5a-ea8e7a0be203",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_PATH = \"./Data/kits19/data\"\n",
    "VALIDATION_DATASET_PATH = \"./Data/kits19/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7732b9ff-88cd-4ac3-b4e9-b76ce82e96f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "img_list = sorted(glob.glob(TRAIN_DATASET_PATH+'/*/*imaging.nii.gz'))\n",
    "seg_list = sorted(glob.glob(TRAIN_DATASET_PATH+'/*/*segmentation.nii.gz'))\n",
    "print(len(img_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae7dae31-aa7f-4b0a-8f1e-8afc4d485e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CNA3d(nn.Module): # conv + norm + activation\n",
    "    def __init__(self, in_channels, out_channels, kSize, stride, padding=(1,1,1), bias=True, norm_args=None, activation_args=None):\n",
    "        super().__init__()\n",
    "        self.norm_args = norm_args\n",
    "        self.activation_args = activation_args\n",
    "        \n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=kSize, stride=stride, padding=padding, bias=bias)\n",
    "\n",
    "        if norm_args is not None:\n",
    "            self.norm = nn.InstanceNorm3d(out_channels, **norm_args)\n",
    "\n",
    "        if activation_args is not None:\n",
    "            self.activation = nn.LeakyReLU(**activation_args)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "\n",
    "        if self.norm_args is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        if self.activation_args is not None:\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class CB3d(nn.Module): # conv block 3d\n",
    "    def __init__(self, in_channels, out_channels, kSize=(3,3), stride=(1,1), padding=(1,1,1), bias=True,\n",
    "                 norm_args:tuple=(None,None), activation_args:tuple=(None,None)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = CNA3d(in_channels, out_channels, kSize=kSize[0], stride=stride[0],\n",
    "                             padding=padding, bias=bias, norm_args=norm_args[0], activation_args=activation_args[0])\n",
    "\n",
    "        self.conv2 = CNA3d(out_channels, out_channels,kSize=kSize[1], stride=stride[1],\n",
    "                             padding=padding, bias=bias, norm_args=norm_args[1], activation_args=activation_args[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BasicNet(nn.Module):\n",
    "    norm_kwargs = {'affine': True}\n",
    "    activation_kwargs = {'negative_slope': 1e-2, 'inplace': True}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BasicNet, self).__init__()\n",
    "\n",
    "    def parameter_count(self):\n",
    "        print(\"model have {} paramerters in total\".format(sum(x.numel() for x in self.parameters()) / 1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0b1cd9b-ca0b-4077-9961-e93021f78ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1, 3, 19, 255, 256]), torch.Size([1, 3, 9, 127, 128]), torch.Size([1, 3, 19, 127, 128]), torch.Size([1, 3, 4, 63, 64]), torch.Size([1, 3, 19, 63, 64]), torch.Size([1, 3, 4, 31, 32]), torch.Size([1, 3, 19, 31, 32])]\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "#from basic_module import *\n",
    "\n",
    "\n",
    "def FMU(x1, x2, mode='sub'):\n",
    "    \"\"\"\n",
    "    feature merging unit\n",
    "    Args:\n",
    "        x1:\n",
    "        x2:\n",
    "        mode: type of fusion\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if mode == 'sum':\n",
    "        return torch.add(x1, x2)\n",
    "    elif mode == 'sub':\n",
    "        return torch.abs(x1 - x2)\n",
    "    elif mode == 'cat':\n",
    "        return torch.cat((x1, x2), dim=1)\n",
    "    else:\n",
    "        raise Exception('Unexpected mode')\n",
    "\n",
    "\n",
    "class Down(BasicNet):\n",
    "    def __init__(self, in_channels, out_channels, mode: tuple, FMU='sub', downsample=True, min_z=8):\n",
    "        \"\"\"\n",
    "        basic module at downsampling stage\n",
    "        Args:\n",
    "            in_channels:\n",
    "            out_channels:\n",
    "            mode: represent the streams coming in and out. e.g., ('2d', 'both'): one input stream (2d) and two output streams (2d and 3d)\n",
    "            FMU: determine the type of feature fusion if there are two input streams\n",
    "            downsample: determine whether to downsample input features (only the first module of MNet do not downsample)\n",
    "            min_z: if the size of z-axis < min_z, maxpooling won't be applied along z-axis\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mode_in, self.mode_out = mode\n",
    "        self.downsample = downsample\n",
    "        self.FMU = FMU\n",
    "        self.min_z = min_z\n",
    "        norm_args = (self.norm_kwargs, self.norm_kwargs)\n",
    "        activation_args = (self.activation_kwargs, self.activation_kwargs)\n",
    "\n",
    "        if self.mode_out == '2d' or self.mode_out == 'both':\n",
    "            self.CB2d = CB3d(in_channels=in_channels, out_channels=out_channels,\n",
    "                             kSize=((1, 3, 3), (1, 3, 3)), stride=(1, 1), padding=(0, 1, 1),\n",
    "                             norm_args=norm_args, activation_args=activation_args)\n",
    "\n",
    "        if self.mode_out == '3d' or self.mode_out == 'both':\n",
    "            self.CB3d = CB3d(in_channels=in_channels, out_channels=out_channels,\n",
    "                             kSize=(3, 3), stride=(1, 1), padding=(1, 1, 1),\n",
    "                             norm_args=norm_args, activation_args=activation_args)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample:\n",
    "            if self.mode_in == 'both':\n",
    "                x2d, x3d = x\n",
    "                p2d = F.max_pool3d(x2d, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "                if x3d.shape[2] >= self.min_z:\n",
    "                    p3d = F.max_pool3d(x3d, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "                else:\n",
    "                    p3d = F.max_pool3d(x3d, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "\n",
    "                x = FMU(p2d, p3d, mode=self.FMU)\n",
    "\n",
    "            elif self.mode_in == '2d':\n",
    "                x = F.max_pool3d(x, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "\n",
    "            elif self.mode_in == '3d':\n",
    "                if x.shape[2] >= self.min_z:\n",
    "                    x = F.max_pool3d(x, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "                else:\n",
    "                    x = F.max_pool3d(x, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "\n",
    "        if self.mode_out == '2d':\n",
    "            return self.CB2d(x)\n",
    "        elif self.mode_out == '3d':\n",
    "            return self.CB3d(x)\n",
    "        elif self.mode_out == 'both':\n",
    "            return self.CB2d(x), self.CB3d(x)\n",
    "\n",
    "\n",
    "\n",
    "class Up(BasicNet):\n",
    "    def __init__(self, in_channels, out_channels, mode: tuple, FMU='sub'):\n",
    "        \"\"\"\n",
    "        basic module at upsampling stage\n",
    "        Args:\n",
    "            in_channels:\n",
    "            out_channels:\n",
    "            mode: represent the streams coming in and out. e.g., ('2d', 'both'): one input stream (2d) and two output streams (2d and 3d)\n",
    "            FMU: determine the type of feature fusion if there are two input streams\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mode_in, self.mode_out = mode\n",
    "        self.FMU = FMU\n",
    "        norm_args = (self.norm_kwargs, self.norm_kwargs)\n",
    "        activation_args = (self.activation_kwargs, self.activation_kwargs)\n",
    "\n",
    "        if self.mode_out == '2d' or self.mode_out == 'both':\n",
    "            self.CB2d = CB3d(in_channels=in_channels, out_channels=out_channels,\n",
    "                             kSize=((1, 3, 3), (1, 3, 3)), stride=(1, 1), padding=(0, 1, 1),\n",
    "                             norm_args=norm_args, activation_args=activation_args)\n",
    "\n",
    "        if self.mode_out == '3d' or self.mode_out == 'both':\n",
    "            self.CB3d = CB3d(in_channels=in_channels, out_channels=out_channels,\n",
    "                             kSize=(3, 3), stride=(1, 1), padding=(1, 1, 1),\n",
    "                             norm_args=norm_args, activation_args=activation_args)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x2d, xskip2d, x3d, xskip3d = x\n",
    "\n",
    "        tarSize = xskip2d.shape[2:]\n",
    "        up2d = F.interpolate(x2d, size=tarSize, mode='trilinear', align_corners=False)\n",
    "        up3d = F.interpolate(x3d, size=tarSize, mode='trilinear', align_corners=False)\n",
    "\n",
    "        cat = torch.cat([FMU(xskip2d, xskip3d, self.FMU), FMU(up2d, up3d, self.FMU)], dim=1)\n",
    "\n",
    "        if self.mode_out == '2d':\n",
    "            return self.CB2d(cat)\n",
    "        elif self.mode_out == '3d':\n",
    "            return self.CB3d(cat)\n",
    "        elif self.mode_out == 'both':\n",
    "            return self.CB2d(cat), self.CB3d(cat)\n",
    "\n",
    "\n",
    "\n",
    "class MNet(BasicNet):\n",
    "    def __init__(self, in_channels, num_classes, kn=(32, 48, 64, 80, 96), ds=True, FMU='sub'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: channels of input\n",
    "            num_classes: output classes\n",
    "            kn: the number of kernels\n",
    "            ds: deep supervision\n",
    "            FMU: type of feature merging unit\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        channel_factor = {'sum': 1, 'sub': 1, 'cat': 2}\n",
    "        fct = channel_factor[FMU]\n",
    "\n",
    "        self.down11 = Down(in_channels, kn[0], ('/', 'both'), downsample=False)\n",
    "        self.down12 = Down(kn[0], kn[1], ('2d', 'both'))\n",
    "        self.down13 = Down(kn[1], kn[2], ('2d', 'both'))\n",
    "        self.down14 = Down(kn[2], kn[3], ('2d', 'both'))\n",
    "        self.bottleneck1 = Down(kn[3], kn[4], ('2d', '2d'))\n",
    "        self.up11 = Up(fct * (kn[3] + kn[4]), kn[3], ('both', '2d'), FMU)\n",
    "        self.up12 = Up(fct * (kn[2] + kn[3]), kn[2], ('both', '2d'), FMU)\n",
    "        self.up13 = Up(fct * (kn[1] + kn[2]), kn[1], ('both', '2d'), FMU)\n",
    "        self.up14 = Up(fct * (kn[0] + kn[1]), kn[0], ('both', 'both'), FMU)\n",
    "\n",
    "        self.down21 = Down(kn[0], kn[1], ('3d', 'both'))\n",
    "        self.down22 = Down(fct * kn[1], kn[2], ('both', 'both'), FMU)\n",
    "        self.down23 = Down(fct * kn[2], kn[3], ('both', 'both'), FMU)\n",
    "        self.bottleneck2 = Down(fct * kn[3], kn[4], ('both', 'both'), FMU)\n",
    "        self.up21 = Up(fct * (kn[3] + kn[4]), kn[3], ('both', 'both'), FMU)\n",
    "        self.up22 = Up(fct * (kn[2] + kn[3]), kn[2], ('both', 'both'), FMU)\n",
    "        self.up23 = Up(fct * (kn[1] + kn[2]), kn[1], ('both', '3d'), FMU)\n",
    "\n",
    "        self.down31 = Down(kn[1], kn[2], ('3d', 'both'))\n",
    "        self.down32 = Down(fct * kn[2], kn[3], ('both', 'both'), FMU)\n",
    "        self.bottleneck3 = Down(fct * kn[3], kn[4], ('both', 'both'), FMU)\n",
    "        self.up31 = Up(fct * (kn[3] + kn[4]), kn[3], ('both', 'both'), FMU)\n",
    "        self.up32 = Up(fct * (kn[2] + kn[3]), kn[2], ('both', '3d'), FMU)\n",
    "\n",
    "        self.down41 = Down(kn[2], kn[3], ('3d', 'both'), FMU)\n",
    "        self.bottleneck4 = Down(fct * kn[3], kn[4], ('both', 'both'), FMU)\n",
    "        self.up41 = Up(fct * (kn[3] + kn[4]), kn[3], ('both', '3d'), FMU)\n",
    "\n",
    "        self.bottleneck5 = Down(kn[3], kn[4], ('3d', '3d'))\n",
    "\n",
    "\n",
    "        self.outputs = nn.ModuleList(\n",
    "            [nn.Conv3d(c, num_classes, kernel_size=(1, 1, 1), stride=1, padding=0, bias=False)\n",
    "             for c in [kn[0], kn[1], kn[1], kn[2], kn[2], kn[3], kn[3]]]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        down11 = self.down11(x)\n",
    "        down12 = self.down12(down11[0])\n",
    "        down13 = self.down13(down12[0])\n",
    "        down14 = self.down14(down13[0])\n",
    "        bottleNeck1 = self.bottleneck1(down14[0])\n",
    "\n",
    "        down21 = self.down21(down11[1])\n",
    "        down22 = self.down22([down21[0], down12[1]])\n",
    "        down23 = self.down23([down22[0], down13[1]])\n",
    "        bottleNeck2 = self.bottleneck2([down23[0], down14[1]])\n",
    "\n",
    "        down31 = self.down31(down21[1])\n",
    "        down32 = self.down32([down31[0], down22[1]])\n",
    "        bottleNeck3 = self.bottleneck3([down32[0], down23[1]])\n",
    "\n",
    "        down41 = self.down41(down31[1])\n",
    "        bottleNeck4 = self.bottleneck4([down41[0], down32[1]])\n",
    "\n",
    "        bottleNeck5 = self.bottleneck5(down41[1])\n",
    "\n",
    "        up41 = self.up41([bottleNeck4[0], down41[0], bottleNeck5, down41[1]])\n",
    "\n",
    "        up31 = self.up31([bottleNeck3[0], down32[0], bottleNeck4[1], down32[1]])\n",
    "        up32 = self.up32([up31[0], down31[0], up41, down31[1]])\n",
    "\n",
    "        up21 = self.up21([bottleNeck2[0], down23[0], bottleNeck3[1], down23[1]])\n",
    "        up22 = self.up22([up21[0], down22[0], up31[1], down22[1]])\n",
    "        up23 = self.up23([up22[0], down21[0], up32, down21[1]])\n",
    "        \n",
    "        up11 = self.up11([bottleNeck1, down14[0], bottleNeck2[1], down14[1]])\n",
    "        up12 = self.up12([up11, down13[0], up21[1], down13[1]])\n",
    "        up13 = self.up13([up12, down12[0], up22[1], down12[1]])\n",
    "        up14 = self.up14([up13, down11[0], up23, down11[1]])\n",
    "\n",
    "\n",
    "        if self.ds:\n",
    "            features = [up14[0]+up14[1], up23, up13, up32, up12, up41,up11]\n",
    "            return [self.outputs[i](features[i]) for i in range(7)]\n",
    "        else:\n",
    "            return self.outputs[0](up14[0]+up14[1])\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mn = MNet(1, 3, kn=(2, 2, 2, 2, 2), ds=True, FMU='sub')\n",
    "    input = torch.randn((1, 1, 19, 255,256))\n",
    "    output = mn(input)\n",
    "\n",
    "    print([e.shape for e in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57aa1e76-f143-4726-a20d-0a3853047125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class KitsDataset(Dataset):\n",
    "    def __init__(self, img, seg):\n",
    "        '''\n",
    "        TODO: Store `seqs`. to `self.x` and `hfs` to `self.y`.\n",
    "\n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        Do NOT permute the data.\n",
    "        '''\n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "    \n",
    "        self.img = img\n",
    "        self.seg = seg\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        TODO: Return the number of samples (i.e. patients).\n",
    "        '''\n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "        return len(self.img)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        TODO: Generates one sample of data.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        '''\n",
    "        return self.img[index], self.seg[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e51c8855-9d51-4b72-86c1-856262228fd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torchmetrics\u001b[38;5;241m.\u001b[39mDice()\n\u001b[0;32m----> 2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01ae2b91-d7d5-41a4-b794-4958b2cfe297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Data/kits19/data/case_00010/imaging.nii.gz\n",
      "./Data/kits19/data/case_00011/imaging.nii.gz\n",
      "./Data/kits19/data/case_00012/imaging.nii.gz\n",
      "./Data/kits19/data/case_00013/imaging.nii.gz\n",
      "./Data/kits19/data/case_00014/imaging.nii.gz\n"
     ]
    }
   ],
   "source": [
    "im_list = []\n",
    "sg_list = []\n",
    "\n",
    "for i in range(10,15):#range(len(t2_list)):\n",
    "    print(img_list[i])\n",
    "    temp_img = nib.load(img_list[i]).get_fdata()\n",
    "    temp_img = torch.from_numpy(scaler.fit_transform(temp_img.reshape(-1,temp_img.shape[-1])).reshape(temp_img.shape))\n",
    "    #print(temp_img.shape)\n",
    "    #print(temp_img)\n",
    "    temp_seg = nib.load(seg_list[i]).get_fdata()\n",
    "    temp_seg = torch.from_numpy(scaler.fit_transform(temp_seg.reshape(-1,temp_seg.shape[-1])).reshape(temp_seg.shape))\n",
    "    \n",
    "    im_list.append(temp_img)\n",
    "    sg_list.append(temp_seg)\n",
    "    '''\n",
    "    temp_t2 = nib.load(t2_list[i]).get_fdata()\n",
    "    temp_t2 = scaler.fit_transform(temp_t2.reshape(-1,temp_t2.shape[-1])).reshape(temp_t2.shape)\n",
    "    \n",
    "    temp_t1=nib.load(t1ce_list[i]).get_fdata()\n",
    "    temp_t1=scaler.fit_transform(temp_t1.reshape(-1, temp_t1.shape[-1])).reshape(temp_t1.shape)\n",
    "   \n",
    "    temp_flair=nib.load(flair_list[i]).get_fdata()\n",
    "    temp_flair=scaler.fit_transform(temp_flair.reshape(-1, temp_flair.shape[-1])).reshape(temp_flair.shape)\n",
    "        \n",
    "    temp_mask=nib.load(mask_list[i]).get_fdata()\n",
    "    temp_mask=temp_mask.astype(np.uint8)\n",
    "    temp_mask[temp_mask==4] = 3 \n",
    "    \n",
    "    t2_img_list.append(temp_t2)\n",
    "    t1_img_list.append(temp_t1)\n",
    "    flair_img_list.append(temp_flair)\n",
    "    mask_img_list.append(temp_mask)\n",
    "    '''\n",
    "kits_dataset = KitsDataset(im_list, sg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b024051e-aa47-456d-9436-8b3a9e6c5a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-3.7695e+00, -3.9521e+00, -4.7775e+00,  ..., -2.3302e-01,\n",
      "           -1.0149e+00, -2.4387e+00],\n",
      "          [ 9.3621e-01, -5.5647e-04, -1.0856e+00,  ...,  1.3083e+00,\n",
      "            1.9675e-01, -9.5304e-01],\n",
      "          [-8.3016e-02, -1.5582e-01, -1.4258e+00,  ...,  1.3259e+00,\n",
      "           -6.2103e-02, -1.7839e+00],\n",
      "          ...,\n",
      "          [ 4.4792e-01,  1.9461e-02,  1.3964e+00,  ...,  1.3099e+00,\n",
      "            5.3495e-01, -1.1274e-01],\n",
      "          [-6.9509e-01, -9.5879e-01, -5.0957e-01,  ..., -9.4747e-01,\n",
      "           -1.9981e+00, -3.2027e-01],\n",
      "          [ 4.2758e-01,  3.0260e-01,  1.1658e+00,  ...,  1.1104e+00,\n",
      "            3.9253e-01,  3.2506e-01]],\n",
      "\n",
      "         [[-3.2354e+00, -4.1189e+00, -4.8267e+00,  ..., -1.1367e+00,\n",
      "           -4.2334e-01,  3.5502e-01],\n",
      "          [ 2.5624e-01,  1.1074e+00,  1.0721e+00,  ...,  6.5080e-01,\n",
      "           -4.3853e-02,  5.0463e-01],\n",
      "          [-4.4199e-02,  5.9178e-02, -8.6187e-03,  ...,  5.9317e-02,\n",
      "           -9.6594e-01,  9.0489e-02],\n",
      "          ...,\n",
      "          [-3.1356e-01,  3.4683e-02,  1.6736e-02,  ...,  6.1298e-01,\n",
      "           -5.6397e-01, -1.7180e+00],\n",
      "          [-2.8466e-01, -5.7427e-01,  2.8057e-02,  ..., -1.2992e+00,\n",
      "           -2.5677e+00, -1.7235e+00],\n",
      "          [ 1.1121e-01,  8.7509e-01,  2.9095e-01,  ...,  1.6589e+00,\n",
      "            6.6037e-01,  8.9413e-01]],\n",
      "\n",
      "         [[-2.5906e+00, -3.3581e+00, -3.7986e+00,  ..., -1.2325e+00,\n",
      "           -1.2214e+00, -1.1924e+00],\n",
      "          [ 5.0121e-01, -5.4785e-01, -7.0563e-01,  ...,  1.4486e+00,\n",
      "            8.3706e-01,  4.3004e-01],\n",
      "          [ 3.1610e-02,  2.3169e-01,  4.1565e-02,  ...,  6.0619e-01,\n",
      "            1.9129e-01, -1.0063e+00],\n",
      "          ...,\n",
      "          [-4.4236e-01,  2.9439e-01,  6.8325e-01,  ...,  1.5406e+00,\n",
      "            6.7697e-01, -7.0961e-02],\n",
      "          [-8.4111e-01, -1.0114e+00, -3.1379e-01,  ..., -1.2749e+00,\n",
      "           -2.4105e+00, -6.8925e-01],\n",
      "          [ 3.2045e-01,  8.4180e-01,  7.7919e-01,  ...,  2.0988e+00,\n",
      "            1.0673e+00,  7.9816e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.3310e+00, -3.3389e+00, -4.1644e+00,  ..., -1.0187e+00,\n",
      "           -1.0369e+00, -1.7733e+00],\n",
      "          [ 5.8024e-01, -4.2805e-01, -1.0914e+00,  ...,  1.3604e+00,\n",
      "            8.7635e-01,  1.3032e-01],\n",
      "          [-1.0628e-01,  6.2809e-03,  4.0866e-02,  ...,  1.6736e+00,\n",
      "            8.7827e-01, -3.6383e-01],\n",
      "          ...,\n",
      "          [-6.7301e-01, -3.5134e-02, -1.3237e-01,  ...,  9.4848e-01,\n",
      "            1.3335e-01, -4.0030e-01],\n",
      "          [-1.2964e+00, -1.9224e+00, -1.1662e+00,  ..., -1.3323e+00,\n",
      "           -3.0703e+00, -1.3672e+00],\n",
      "          [-5.4393e-01,  3.0193e-01, -2.4196e-01,  ...,  1.3441e+00,\n",
      "            8.2016e-01,  8.7946e-01]],\n",
      "\n",
      "         [[-3.8926e+00, -4.3296e+00, -5.8773e+00,  ..., -9.4002e-01,\n",
      "           -9.9073e-01, -7.6547e-01],\n",
      "          [ 1.8798e-02,  9.3117e-01,  1.0971e+00,  ...,  1.2386e+00,\n",
      "            9.4587e-01,  6.5369e-01],\n",
      "          [-6.2644e-02,  6.1884e-01,  9.6940e-01,  ...,  1.2157e+00,\n",
      "            3.0452e-01, -9.1094e-01],\n",
      "          ...,\n",
      "          [ 2.5121e-04,  5.1409e-01,  9.8622e-01,  ...,  2.2240e+00,\n",
      "            1.0025e+00, -2.6602e+00],\n",
      "          [-7.3066e-01, -8.1239e-01,  7.6008e-01,  ...,  1.9015e-01,\n",
      "           -1.6492e+00, -2.2111e+00],\n",
      "          [ 5.2774e-01,  1.1080e+00,  1.1296e+00,  ...,  2.6612e+00,\n",
      "            1.3365e+00,  1.4080e+00]],\n",
      "\n",
      "         [[-3.1434e+00, -2.1669e+00, -2.7862e+00,  ...,  1.8011e-03,\n",
      "            2.7346e-01,  2.5784e-01],\n",
      "          [ 5.4528e-01, -3.8301e-02, -1.2172e+00,  ..., -1.7142e+00,\n",
      "           -2.8252e+00, -1.1464e+00],\n",
      "          [-1.2452e-02, -7.9526e-01, -3.4221e+00,  ..., -1.9749e-01,\n",
      "           -5.5345e-01,  1.6888e-01],\n",
      "          ...,\n",
      "          [-1.9484e-01, -3.9977e-01, -2.1176e+00,  ..., -9.7654e-01,\n",
      "           -6.1110e-01, -3.2770e+00],\n",
      "          [-9.1207e-01, -9.1428e-01, -1.8091e+00,  ..., -1.2441e+00,\n",
      "           -7.3831e-01, -3.1444e+00],\n",
      "          [-4.8693e-01, -2.5910e-01, -6.5000e-01,  ..., -4.1554e-01,\n",
      "           -3.7008e-01,  5.6010e-01]]]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([-4.0403, -4.2355, -5.0852,  ..., -2.3350, -0.4784, -1.8682],\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#print(mask)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred,mask\u001b[38;5;241m.\u001b[39mint())\n\u001b[0;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#print(output)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(kits_dataset,batch_size=1)\n",
    "mn = MNet(1, 3, kn=(2, 2, 2, 2, 2), ds=False, FMU='sub').float()\n",
    "criterion = torchmetrics.Dice()\n",
    "optimizer = torch.optim.SGD(mn.parameters(), lr=0.001) \n",
    "assert isinstance(optimizer, torch.optim.SGD)\n",
    "\n",
    "\n",
    "mn.train()\n",
    "for (img,mask) in dataloader: \n",
    "    #print(type(t2))\n",
    "    #print(torch.unsqueeze(t2.float(),dim=0).size())\n",
    "    #print(img)\n",
    "    optimizer.zero_grad()\n",
    "    output = mn(torch.unsqueeze(img.float(),dim=0))\n",
    "    pred = torch.unsqueeze(torch.sum(output[0],dim=0),dim=0)\n",
    "    #print(len(output))\n",
    "    #print(torch.unsqueeze(torch.sum(output[0],dim=0),dim=0).size())\n",
    "    #print(mask.size())\n",
    "    #print(mask[mask>0])\n",
    "    print(pred)\n",
    "    pred[pred<0] = 0\n",
    "    print(output[0][output[0]<0])\n",
    "    #print(mask)\n",
    "    loss = criterion(pred,mask.int())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
