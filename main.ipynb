{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "259f08ab-ef8f-4a7c-954d-70c20604b67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nibabel in /usr/local/lib/python3.9/dist-packages (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.9/dist-packages (from nibabel) (1.23.4)\n",
      "Requirement already satisfied: packaging>=17 in /usr/local/lib/python3.9/dist-packages (from nibabel) (23.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torchmetrics\n",
      "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.23.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n",
      "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.12.1+cu116)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.4.0)\n",
      "Installing collected packages: torchmetrics\n",
      "Successfully installed torchmetrics-0.11.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nibabel\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0178a542-fb3a-491a-a89c-9dbc41d89b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import glob\n",
    "import os\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imsave\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torchmetrics import Dice\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ca27e11-841c-47ef-b6bc-e178d5a9cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_PATH = \"./Data/brats20\"\n",
    "VALIDATION_DATASET_PATH = \"./Data/brats20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d822ca23-59cd-4519-8ae1-e3f00aa89b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_list = sorted(glob.glob(TRAIN_DATASET_PATH+'/*/*t2.nii'))\n",
    "t1ce_list = sorted(glob.glob(TRAIN_DATASET_PATH+'/*/*t1ce.nii'))\n",
    "flair_list = sorted(glob.glob(TRAIN_DATASET_PATH+'/*/*flair.nii'))\n",
    "mask_list = sorted(glob.glob(TRAIN_DATASET_PATH+'/*/*[se][eg][gm].nii')) # handling the 355th file also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7c75fae-c601-4437-8a5e-f732c4cd891e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Data/brats20/BraTS20_Training_001/BraTS20_Training_001_t2.nii\n",
      "./Data/brats20/BraTS20_Training_002/BraTS20_Training_002_t2.nii\n",
      "./Data/brats20/BraTS20_Training_003/BraTS20_Training_003_t2.nii\n",
      "./Data/brats20/BraTS20_Training_004/BraTS20_Training_004_t2.nii\n",
      "./Data/brats20/BraTS20_Training_005/BraTS20_Training_005_t2.nii\n",
      "./Data/brats20/BraTS20_Training_006/BraTS20_Training_006_t2.nii\n",
      "./Data/brats20/BraTS20_Training_007/BraTS20_Training_007_t2.nii\n",
      "./Data/brats20/BraTS20_Training_008/BraTS20_Training_008_t2.nii\n",
      "./Data/brats20/BraTS20_Training_009/BraTS20_Training_009_t2.nii\n",
      "./Data/brats20/BraTS20_Training_010/BraTS20_Training_010_t2.nii\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BratsDataset(Dataset):\n",
    "    def __init__(self,t2s,t1s,flairs,masks):\n",
    "        self.t2 = t2s\n",
    "        self.t1 = t1s\n",
    "        self.flair = flairs\n",
    "        self.mask = masks\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.t2)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return self.t2[index],self.t1[index],self.flair[index],self.mask[index]\n",
    "    \n",
    "t2_img_list = []\n",
    "t1_img_list = []\n",
    "flair_img_list = []\n",
    "mask_img_list = []\n",
    "\n",
    "for i in range(0,10):#range(len(t2_list)):\n",
    "    print(t2_list[i])\n",
    "    temp_t2 = nib.load(t2_list[i]).get_fdata()\n",
    "    temp_t2 = scaler.fit_transform(temp_t2.reshape(-1,temp_t2.shape[-1])).reshape(temp_t2.shape)\n",
    "    \n",
    "    temp_t1=nib.load(t1ce_list[i]).get_fdata()\n",
    "    temp_t1=scaler.fit_transform(temp_t1.reshape(-1, temp_t1.shape[-1])).reshape(temp_t1.shape)\n",
    "   \n",
    "    temp_flair=nib.load(flair_list[i]).get_fdata()\n",
    "    temp_flair=scaler.fit_transform(temp_flair.reshape(-1, temp_flair.shape[-1])).reshape(temp_flair.shape)\n",
    "        \n",
    "    temp_mask=nib.load(mask_list[i]).get_fdata()\n",
    "    temp_mask=temp_mask.astype(np.uint8)\n",
    "    temp_mask[temp_mask==4] = 3 \n",
    "    \n",
    "    t2_img_list.append(temp_t2)\n",
    "    t1_img_list.append(temp_t1)\n",
    "    flair_img_list.append(temp_flair)\n",
    "    mask_img_list.append(temp_mask)\n",
    "    \n",
    "brats_dataset = BratsDataset(t2_img_list,t1_img_list,flair_img_list,mask_img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d554a01b-816d-4645-bf1c-2c422dcf3d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CNA3d(nn.Module): # conv + norm + activation\n",
    "    def __init__(self, in_channels, out_channels, kSize, stride, padding=(1,1,1), bias=True, norm_args=None, activation_args=None):\n",
    "        super().__init__()\n",
    "        self.norm_args = norm_args\n",
    "        self.activation_args = activation_args\n",
    "        \n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=kSize, stride=stride, padding=padding, bias=bias)\n",
    "\n",
    "        if norm_args is not None:\n",
    "            self.norm = nn.InstanceNorm3d(out_channels, **norm_args)\n",
    "\n",
    "        if activation_args is not None:\n",
    "            self.activation = nn.LeakyReLU(**activation_args)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "\n",
    "        if self.norm_args is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        if self.activation_args is not None:\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class CB3d(nn.Module): # conv block 3d\n",
    "    def __init__(self, in_channels, out_channels, kSize=(3,3), stride=(1,1), padding=(1,1,1), bias=True,\n",
    "                 norm_args:tuple=(None,None), activation_args:tuple=(None,None)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = CNA3d(in_channels, out_channels, kSize=kSize[0], stride=stride[0],\n",
    "                             padding=padding, bias=bias, norm_args=norm_args[0], activation_args=activation_args[0])\n",
    "\n",
    "        self.conv2 = CNA3d(out_channels, out_channels,kSize=kSize[1], stride=stride[1],\n",
    "                             padding=padding, bias=bias, norm_args=norm_args[1], activation_args=activation_args[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BasicNet(nn.Module):\n",
    "    norm_kwargs = {'affine': True}\n",
    "    activation_kwargs = {'negative_slope': 1e-2, 'inplace': True}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BasicNet, self).__init__()\n",
    "\n",
    "    def parameter_count(self):\n",
    "        print(\"model have {} paramerters in total\".format(sum(x.numel() for x in self.parameters()) / 1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff2aea6a-6219-4bd7-b09f-61c789b0fe7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1, 3, 19, 255, 256]), torch.Size([1, 3, 9, 127, 128]), torch.Size([1, 3, 19, 127, 128]), torch.Size([1, 3, 4, 63, 64]), torch.Size([1, 3, 19, 63, 64]), torch.Size([1, 3, 4, 31, 32]), torch.Size([1, 3, 19, 31, 32])]\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "#from basic_module import *\n",
    "\n",
    "\n",
    "def FMU(x1, x2, mode='sub'):\n",
    "    \"\"\"\n",
    "    feature merging unit\n",
    "    Args:\n",
    "        x1:\n",
    "        x2:\n",
    "        mode: type of fusion\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if mode == 'sum':\n",
    "        return torch.add(x1, x2)\n",
    "    elif mode == 'sub':\n",
    "        return torch.abs(x1 - x2)\n",
    "    elif mode == 'cat':\n",
    "        return torch.cat((x1, x2), dim=1)\n",
    "    else:\n",
    "        raise Exception('Unexpected mode')\n",
    "\n",
    "\n",
    "class Down(BasicNet):\n",
    "    def __init__(self, in_channels, out_channels, mode: tuple, FMU='sub', downsample=True, min_z=8):\n",
    "        \"\"\"\n",
    "        basic module at downsampling stage\n",
    "        Args:\n",
    "            in_channels:\n",
    "            out_channels:\n",
    "            mode: represent the streams coming in and out. e.g., ('2d', 'both'): one input stream (2d) and two output streams (2d and 3d)\n",
    "            FMU: determine the type of feature fusion if there are two input streams\n",
    "            downsample: determine whether to downsample input features (only the first module of MNet do not downsample)\n",
    "            min_z: if the size of z-axis < min_z, maxpooling won't be applied along z-axis\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mode_in, self.mode_out = mode\n",
    "        self.downsample = downsample\n",
    "        self.FMU = FMU\n",
    "        self.min_z = min_z\n",
    "        norm_args = (self.norm_kwargs, self.norm_kwargs)\n",
    "        activation_args = (self.activation_kwargs, self.activation_kwargs)\n",
    "\n",
    "        if self.mode_out == '2d' or self.mode_out == 'both':\n",
    "            self.CB2d = CB3d(in_channels=in_channels, out_channels=out_channels,\n",
    "                             kSize=((1, 3, 3), (1, 3, 3)), stride=(1, 1), padding=(0, 1, 1),\n",
    "                             norm_args=norm_args, activation_args=activation_args)\n",
    "\n",
    "        if self.mode_out == '3d' or self.mode_out == 'both':\n",
    "            self.CB3d = CB3d(in_channels=in_channels, out_channels=out_channels,\n",
    "                             kSize=(3, 3), stride=(1, 1), padding=(1, 1, 1),\n",
    "                             norm_args=norm_args, activation_args=activation_args)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample:\n",
    "            if self.mode_in == 'both':\n",
    "                x2d, x3d = x\n",
    "                p2d = F.max_pool3d(x2d, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "                if x3d.shape[2] >= self.min_z:\n",
    "                    p3d = F.max_pool3d(x3d, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "                else:\n",
    "                    p3d = F.max_pool3d(x3d, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "\n",
    "                x = FMU(p2d, p3d, mode=self.FMU)\n",
    "\n",
    "            elif self.mode_in == '2d':\n",
    "                x = F.max_pool3d(x, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "\n",
    "            elif self.mode_in == '3d':\n",
    "                if x.shape[2] >= self.min_z:\n",
    "                    x = F.max_pool3d(x, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "                else:\n",
    "                    x = F.max_pool3d(x, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "\n",
    "        if self.mode_out == '2d':\n",
    "            return self.CB2d(x)\n",
    "        elif self.mode_out == '3d':\n",
    "            return self.CB3d(x)\n",
    "        elif self.mode_out == 'both':\n",
    "            return self.CB2d(x), self.CB3d(x)\n",
    "\n",
    "\n",
    "\n",
    "class Up(BasicNet):\n",
    "    def __init__(self, in_channels, out_channels, mode: tuple, FMU='sub'):\n",
    "        \"\"\"\n",
    "        basic module at upsampling stage\n",
    "        Args:\n",
    "            in_channels:\n",
    "            out_channels:\n",
    "            mode: represent the streams coming in and out. e.g., ('2d', 'both'): one input stream (2d) and two output streams (2d and 3d)\n",
    "            FMU: determine the type of feature fusion if there are two input streams\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mode_in, self.mode_out = mode\n",
    "        self.FMU = FMU\n",
    "        norm_args = (self.norm_kwargs, self.norm_kwargs)\n",
    "        activation_args = (self.activation_kwargs, self.activation_kwargs)\n",
    "\n",
    "        if self.mode_out == '2d' or self.mode_out == 'both':\n",
    "            self.CB2d = CB3d(in_channels=in_channels, out_channels=out_channels,\n",
    "                             kSize=((1, 3, 3), (1, 3, 3)), stride=(1, 1), padding=(0, 1, 1),\n",
    "                             norm_args=norm_args, activation_args=activation_args)\n",
    "\n",
    "        if self.mode_out == '3d' or self.mode_out == 'both':\n",
    "            self.CB3d = CB3d(in_channels=in_channels, out_channels=out_channels,\n",
    "                             kSize=(3, 3), stride=(1, 1), padding=(1, 1, 1),\n",
    "                             norm_args=norm_args, activation_args=activation_args)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x2d, xskip2d, x3d, xskip3d = x\n",
    "\n",
    "        tarSize = xskip2d.shape[2:]\n",
    "        up2d = F.interpolate(x2d, size=tarSize, mode='trilinear', align_corners=False)\n",
    "        up3d = F.interpolate(x3d, size=tarSize, mode='trilinear', align_corners=False)\n",
    "\n",
    "        cat = torch.cat([FMU(xskip2d, xskip3d, self.FMU), FMU(up2d, up3d, self.FMU)], dim=1)\n",
    "\n",
    "        if self.mode_out == '2d':\n",
    "            return self.CB2d(cat)\n",
    "        elif self.mode_out == '3d':\n",
    "            return self.CB3d(cat)\n",
    "        elif self.mode_out == 'both':\n",
    "            return self.CB2d(cat), self.CB3d(cat)\n",
    "\n",
    "\n",
    "\n",
    "class MNet(BasicNet):\n",
    "    def __init__(self, in_channels, num_classes, kn=(32, 48, 64, 80, 96), ds=True, FMU='sub'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: channels of input\n",
    "            num_classes: output classes\n",
    "            kn: the number of kernels\n",
    "            ds: deep supervision\n",
    "            FMU: type of feature merging unit\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        channel_factor = {'sum': 1, 'sub': 1, 'cat': 2}\n",
    "        fct = channel_factor[FMU]\n",
    "\n",
    "        self.down11 = Down(in_channels, kn[0], ('/', 'both'), downsample=False)\n",
    "        self.down12 = Down(kn[0], kn[1], ('2d', 'both'))\n",
    "        self.down13 = Down(kn[1], kn[2], ('2d', 'both'))\n",
    "        self.down14 = Down(kn[2], kn[3], ('2d', 'both'))\n",
    "        self.bottleneck1 = Down(kn[3], kn[4], ('2d', '2d'))\n",
    "        self.up11 = Up(fct * (kn[3] + kn[4]), kn[3], ('both', '2d'), FMU)\n",
    "        self.up12 = Up(fct * (kn[2] + kn[3]), kn[2], ('both', '2d'), FMU)\n",
    "        self.up13 = Up(fct * (kn[1] + kn[2]), kn[1], ('both', '2d'), FMU)\n",
    "        self.up14 = Up(fct * (kn[0] + kn[1]), kn[0], ('both', 'both'), FMU)\n",
    "\n",
    "        self.down21 = Down(kn[0], kn[1], ('3d', 'both'))\n",
    "        self.down22 = Down(fct * kn[1], kn[2], ('both', 'both'), FMU)\n",
    "        self.down23 = Down(fct * kn[2], kn[3], ('both', 'both'), FMU)\n",
    "        self.bottleneck2 = Down(fct * kn[3], kn[4], ('both', 'both'), FMU)\n",
    "        self.up21 = Up(fct * (kn[3] + kn[4]), kn[3], ('both', 'both'), FMU)\n",
    "        self.up22 = Up(fct * (kn[2] + kn[3]), kn[2], ('both', 'both'), FMU)\n",
    "        self.up23 = Up(fct * (kn[1] + kn[2]), kn[1], ('both', '3d'), FMU)\n",
    "\n",
    "        self.down31 = Down(kn[1], kn[2], ('3d', 'both'))\n",
    "        self.down32 = Down(fct * kn[2], kn[3], ('both', 'both'), FMU)\n",
    "        self.bottleneck3 = Down(fct * kn[3], kn[4], ('both', 'both'), FMU)\n",
    "        self.up31 = Up(fct * (kn[3] + kn[4]), kn[3], ('both', 'both'), FMU)\n",
    "        self.up32 = Up(fct * (kn[2] + kn[3]), kn[2], ('both', '3d'), FMU)\n",
    "\n",
    "        self.down41 = Down(kn[2], kn[3], ('3d', 'both'), FMU)\n",
    "        self.bottleneck4 = Down(fct * kn[3], kn[4], ('both', 'both'), FMU)\n",
    "        self.up41 = Up(fct * (kn[3] + kn[4]), kn[3], ('both', '3d'), FMU)\n",
    "\n",
    "        self.bottleneck5 = Down(kn[3], kn[4], ('3d', '3d'))\n",
    "\n",
    "\n",
    "        self.outputs = nn.ModuleList(\n",
    "            [nn.Conv3d(c, num_classes, kernel_size=(1, 1, 1), stride=1, padding=0, bias=False)\n",
    "             for c in [kn[0], kn[1], kn[1], kn[2], kn[2], kn[3], kn[3]]]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        down11 = self.down11(x)\n",
    "        down12 = self.down12(down11[0])\n",
    "        down13 = self.down13(down12[0])\n",
    "        down14 = self.down14(down13[0])\n",
    "        bottleNeck1 = self.bottleneck1(down14[0])\n",
    "\n",
    "        down21 = self.down21(down11[1])\n",
    "        down22 = self.down22([down21[0], down12[1]])\n",
    "        down23 = self.down23([down22[0], down13[1]])\n",
    "        bottleNeck2 = self.bottleneck2([down23[0], down14[1]])\n",
    "\n",
    "        down31 = self.down31(down21[1])\n",
    "        down32 = self.down32([down31[0], down22[1]])\n",
    "        bottleNeck3 = self.bottleneck3([down32[0], down23[1]])\n",
    "\n",
    "        down41 = self.down41(down31[1])\n",
    "        bottleNeck4 = self.bottleneck4([down41[0], down32[1]])\n",
    "\n",
    "        bottleNeck5 = self.bottleneck5(down41[1])\n",
    "\n",
    "        up41 = self.up41([bottleNeck4[0], down41[0], bottleNeck5, down41[1]])\n",
    "\n",
    "        up31 = self.up31([bottleNeck3[0], down32[0], bottleNeck4[1], down32[1]])\n",
    "        up32 = self.up32([up31[0], down31[0], up41, down31[1]])\n",
    "\n",
    "        up21 = self.up21([bottleNeck2[0], down23[0], bottleNeck3[1], down23[1]])\n",
    "        up22 = self.up22([up21[0], down22[0], up31[1], down22[1]])\n",
    "        up23 = self.up23([up22[0], down21[0], up32, down21[1]])\n",
    "        \n",
    "        up11 = self.up11([bottleNeck1, down14[0], bottleNeck2[1], down14[1]])\n",
    "        up12 = self.up12([up11, down13[0], up21[1], down13[1]])\n",
    "        up13 = self.up13([up12, down12[0], up22[1], down12[1]])\n",
    "        up14 = self.up14([up13, down11[0], up23, down11[1]])\n",
    "\n",
    "\n",
    "        if self.ds:\n",
    "            features = [up14[0]+up14[1], up23, up13, up32, up12, up41,up11]\n",
    "            return [self.outputs[i](features[i]) for i in range(7)]\n",
    "        else:\n",
    "            return self.outputs[0](up14[0]+up14[1])\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mn = MNet(1, 3, kn=(2, 2, 2, 2, 2), ds=True, FMU='sub')\n",
    "    input = torch.randn((1, 1, 19, 255,256))\n",
    "    output = mn(input)\n",
    "\n",
    "    print([e.shape for e in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4855859c-81d7-40f0-993e-448b064d384a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "7\n",
      "torch.Size([1, 3, 19, 255, 256])\n"
     ]
    }
   ],
   "source": [
    "mn = MNet(1, 3, kn=(2, 2, 2, 2, 2), ds=True, FMU='sub')\n",
    "input = torch.randn((1, 1, 19, 255,256))\n",
    "print(input.dtype)\n",
    "input = input.float()\n",
    "print(input.dtype)\n",
    "output = mn(input)\n",
    "print(len(output))\n",
    "print(output[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "982fa029-ef26-40cb-923e-895a6c8ffb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torchmetrics.Dice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69d5337c-a440-4232-9930-cf00a04975e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      2\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(brats_dataset,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m mn \u001b[38;5;241m=\u001b[39m \u001b[43mMNet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFMU\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msub\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      5\u001b[0m mn\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (t2,t1,flair,mask) \u001b[38;5;129;01min\u001b[39;00m dataloader: \n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#print(type(t2))\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#print(torch.unsqueeze(t2.float(),dim=0).size())\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [7], line 149\u001b[0m, in \u001b[0;36mMNet.__init__\u001b[0;34m(self, in_channels, num_classes, kn, ds, FMU)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown13 \u001b[38;5;241m=\u001b[39m Down(kn[\u001b[38;5;241m1\u001b[39m], kn[\u001b[38;5;241m2\u001b[39m], (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2d\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown14 \u001b[38;5;241m=\u001b[39m Down(kn[\u001b[38;5;241m2\u001b[39m], kn[\u001b[38;5;241m3\u001b[39m], (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2d\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbottleneck1 \u001b[38;5;241m=\u001b[39m Down(kn[\u001b[38;5;241m3\u001b[39m], \u001b[43mkn\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2d\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2d\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup11 \u001b[38;5;241m=\u001b[39m Up(fct \u001b[38;5;241m*\u001b[39m (kn[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m kn[\u001b[38;5;241m4\u001b[39m]), kn[\u001b[38;5;241m3\u001b[39m], (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2d\u001b[39m\u001b[38;5;124m'\u001b[39m), FMU)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup12 \u001b[38;5;241m=\u001b[39m Up(fct \u001b[38;5;241m*\u001b[39m (kn[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m kn[\u001b[38;5;241m3\u001b[39m]), kn[\u001b[38;5;241m2\u001b[39m], (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2d\u001b[39m\u001b[38;5;124m'\u001b[39m), FMU)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(brats_dataset,batch_size=1)\n",
    "mn = MNet(1, 3, kn=(2, 2, 4, 2), ds=True, FMU='sub').float()\n",
    "\n",
    "mn.train()\n",
    "for (t2,t1,flair,mask) in dataloader: \n",
    "    #print(type(t2))\n",
    "    #print(torch.unsqueeze(t2.float(),dim=0).size())\n",
    "    optimizer.zero_grad()\n",
    "    output = mn(t2.float())\n",
    "    print(len(output))\n",
    "    print(output[6].size())\n",
    "    print(mask.size())\n",
    "    loss = optimizer(output,mask)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f6ce566-5964-4423-8aa5-80863c576f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "torch.Size([1, 3, 240, 240, 155])\n",
      "torch.Size([1, 3, 240, 240, 155])\n",
      "torch.Size([1, 240, 240, 155])\n",
      "tensor([[[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]]], dtype=torch.uint8)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(mask\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(mask)\n\u001b[0;32m---> 16\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchmetrics/metric.py:236\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_full_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_reduce_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchmetrics/metric.py:302\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# allow grads for batch computation\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# reduce batch and global state\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchmetrics/metric.py:390\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchmetrics/classification/dice.py:200\u001b[0m, in \u001b[0;36mDice.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update state with predictions and targets.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     tp, fp, tn, fn \u001b[38;5;241m=\u001b[39m \u001b[43m_stat_scores_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmdmc_reduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmdmc_reduce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulticlass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulticlass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;66;03m# Update states\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce \u001b[38;5;241m!=\u001b[39m AverageMethod\u001b[38;5;241m.\u001b[39mSAMPLES \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmdmc_reduce \u001b[38;5;241m!=\u001b[39m MDMCAverageMethod\u001b[38;5;241m.\u001b[39mSAMPLEWISE:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchmetrics/functional/classification/stat_scores.py:945\u001b[0m, in \u001b[0;36m_stat_scores_update\u001b[0;34m(preds, target, reduce, mdmc_reduce, num_classes, top_k, threshold, multiclass, ignore_index, mode)\u001b[0m\n\u001b[1;32m    942\u001b[0m     preds, target \u001b[38;5;241m=\u001b[39m _drop_negative_ignored_indices(preds, target, ignore_index, mode)\n\u001b[1;32m    943\u001b[0m     _negative_index_dropped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m preds, target, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_input_format_classification\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulticlass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulticlass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ignore_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m preds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `ignore_index` \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mignore_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid for inputs with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m classes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchmetrics/utilities/checks.py:411\u001b[0m, in \u001b[0;36m_input_format_classification\u001b[0;34m(preds, target, threshold, top_k, num_classes, multiclass, ignore_index)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert preds and target tensors into common format.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03mPreds and targets are supposed to fall into one of these categories (and are\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;124;03m        ``'multi-dim multi-class'``\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# Remove excess dimensions\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m \u001b[43m_input_squeeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# Convert half precision tensors to full precision, as not all ops are supported\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# for example, min() is not supported\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchmetrics/utilities/checks.py:314\u001b[0m, in \u001b[0;36m_input_squeeze\u001b[0;34m(preds, target)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_input_squeeze\u001b[39m(\n\u001b[1;32m    310\u001b[0m     preds: Tensor,\n\u001b[1;32m    311\u001b[0m     target: Tensor,\n\u001b[1;32m    312\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[1;32m    313\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Remove excess dimensions.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    315\u001b[0m         preds, target \u001b[38;5;241m=\u001b[39m preds\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), target\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(brats_dataset,batch_size=1)\n",
    "mn = MNet(1, 3, kn=(2, 2, 2, 2, 2), ds=True, FMU='sub').float()\n",
    "\n",
    "mn.train()\n",
    "for (t2,t1,flair,mask) in dataloader: \n",
    "    #print(type(t2))\n",
    "    #print(torch.unsqueeze(t2.float(),dim=0).size())\n",
    "    optimizer.zero_grad()\n",
    "    output = mn(torch.unsqueeze(t2.float(),dim=0))\n",
    "    print(len(output))\n",
    "    print(output[0].size())\n",
    "    print(torch.squeeze(output[0],dim=1).size())\n",
    "    print(mask.size())\n",
    "    print(mask)\n",
    "    loss = optimizer(output,mask)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
