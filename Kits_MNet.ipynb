{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3f9d95f-9cdb-41aa-bd75-b309b6a231af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nibabel\n",
      "  Downloading nibabel-5.1.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=17 in /usr/local/lib/python3.9/dist-packages (from nibabel) (23.0)\n",
      "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.9/dist-packages (from nibabel) (1.23.4)\n",
      "Installing collected packages: nibabel\n",
      "Successfully installed nibabel-5.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torchmetrics\n",
      "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.12.1+cu116)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.4.0)\n",
      "Installing collected packages: torchmetrics\n",
      "Successfully installed torchmetrics-0.11.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nibabel\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6051c48b-0245-4a91-bf28-7d74d8603921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import glob\n",
    "import os\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imsave\n",
    "import torchmetrics\n",
    "from torchmetrics import Dice\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b72ee882-8b6c-46cf-8a5a-ea8e7a0be203",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_PATH = \"./Data/kits19/data\"\n",
    "VALIDATION_DATASET_PATH = \"./Data/kits19/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7732b9ff-88cd-4ac3-b4e9-b76ce82e96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = sorted(glob.glob(TRAIN_DATASET_PATH+'/*/*imaging.nii.gz'))\n",
    "seg_list = sorted(glob.glob(TRAIN_DATASET_PATH+'/*/*segmentation.nii.gz')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae7dae31-aa7f-4b0a-8f1e-8afc4d485e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CNA3d(nn.Module): # conv + norm + activation\n",
    "    def __init__(self, in_channels, out_channels, kSize, stride, padding=(1,1,1), bias=True, norm_args=None, activation_args=None):\n",
    "        super().__init__()\n",
    "        self.norm_args = norm_args\n",
    "        self.activation_args = activation_args\n",
    "        \n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=kSize, stride=stride, padding=padding, bias=bias)\n",
    "\n",
    "        if norm_args is not None:\n",
    "            self.norm = nn.InstanceNorm3d(out_channels, **norm_args)\n",
    "\n",
    "        if activation_args is not None:\n",
    "            self.activation = nn.LeakyReLU(**activation_args)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "\n",
    "        if self.norm_args is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        if self.activation_args is not None:\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class CB3d(nn.Module): # conv block 3d\n",
    "    def __init__(self, in_channels, out_channels, kSize=(3,3), stride=(1,1), padding=(1,1,1), bias=True,\n",
    "                 norm_args:tuple=(None,None), activation_args:tuple=(None,None)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = CNA3d(in_channels, out_channels, kSize=kSize[0], stride=stride[0],\n",
    "                             padding=padding, bias=bias, norm_args=norm_args[0], activation_args=activation_args[0])\n",
    "\n",
    "        self.conv2 = CNA3d(out_channels, out_channels,kSize=kSize[1], stride=stride[1],\n",
    "                             padding=padding, bias=bias, norm_args=norm_args[1], activation_args=activation_args[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BasicNet(nn.Module):\n",
    "    norm_kwargs = {'affine': True}\n",
    "    activation_kwargs = {'negative_slope': 1e-2, 'inplace': True}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BasicNet, self).__init__()\n",
    "\n",
    "    def parameter_count(self):\n",
    "        print(\"model have {} paramerters in total\".format(sum(x.numel() for x in self.parameters()) / 1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0b1cd9b-ca0b-4077-9961-e93021f78ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "#from basic_module import *\n",
    "\n",
    "\n",
    "def FMU(x1, x2, mode='sub'):\n",
    "    \"\"\"\n",
    "    feature merging unit\n",
    "    Args:\n",
    "        x1:\n",
    "        x2:\n",
    "        mode: type of fusion\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if mode == 'sum':\n",
    "        return torch.add(x1, x2)\n",
    "    elif mode == 'sub':\n",
    "        return torch.abs(x1 - x2)\n",
    "    elif mode == 'cat':\n",
    "        return torch.cat((x1, x2), dim=1)\n",
    "    else:\n",
    "        raise Exception('Unexpected mode')\n",
    "\n",
    "\n",
    "class Down(BasicNet):\n",
    "    def __init__(self, in_channels, out_channels, mode: tuple, FMU='sub', downsample=True, min_z=8):\n",
    "        \"\"\"\n",
    "        basic module at downsampling stage\n",
    "        Args:\n",
    "            in_channels:\n",
    "            out_channels:\n",
    "            mode: represent the streams coming in and out. e.g., ('2d', 'both'): one input stream (2d) and two output streams (2d and 3d)\n",
    "            FMU: determine the type of feature fusion if there are two input streams\n",
    "            downsample: determine whether to downsample input features (only the first module of MNet do not downsample)\n",
    "            min_z: if the size of z-axis < min_z, maxpooling won't be applied along z-axis\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mode_in, self.mode_out = mode\n",
    "        self.downsample = downsample\n",
    "        self.FMU = FMU\n",
    "        self.min_z = min_z\n",
    "        norm_args = (self.norm_kwargs, self.norm_kwargs)\n",
    "        activation_args = (self.activation_kwargs, self.activation_kwargs)\n",
    "\n",
    "        if self.mode_out == '2d' or self.mode_out == 'both':\n",
    "            self.CB2d = CB3d(in_channels=in_channels, out_channels=out_channels,\n",
    "                             kSize=((1, 3, 3), (1, 3, 3)), stride=(1, 1), padding=(0, 1, 1),\n",
    "                             norm_args=norm_args, activation_args=activation_args)\n",
    "\n",
    "        if self.mode_out == '3d' or self.mode_out == 'both':\n",
    "            self.CB3d = CB3d(in_channels=in_channels, out_channels=out_channels,\n",
    "                             kSize=(3, 3), stride=(1, 1), padding=(1, 1, 1),\n",
    "                             norm_args=norm_args, activation_args=activation_args)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample:\n",
    "            if self.mode_in == 'both':\n",
    "                x2d, x3d = x\n",
    "                p2d = F.max_pool3d(x2d, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "                if x3d.shape[2] >= self.min_z:\n",
    "                    p3d = F.max_pool3d(x3d, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "                else:\n",
    "                    p3d = F.max_pool3d(x3d, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "\n",
    "                x = FMU(p2d, p3d, mode=self.FMU)\n",
    "\n",
    "            elif self.mode_in == '2d':\n",
    "                x = F.max_pool3d(x, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "\n",
    "            elif self.mode_in == '3d':\n",
    "                if x.shape[2] >= self.min_z:\n",
    "                    x = F.max_pool3d(x, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "                else:\n",
    "                    x = F.max_pool3d(x, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "\n",
    "        if self.mode_out == '2d':\n",
    "            return self.CB2d(x)\n",
    "        elif self.mode_out == '3d':\n",
    "            return self.CB3d(x)\n",
    "        elif self.mode_out == 'both':\n",
    "            return self.CB2d(x), self.CB3d(x)\n",
    "\n",
    "\n",
    "\n",
    "class Up(BasicNet):\n",
    "    def __init__(self, in_channels, out_channels, mode: tuple, FMU='sub'):\n",
    "        \"\"\"\n",
    "        basic module at upsampling stage\n",
    "        Args:\n",
    "            in_channels:\n",
    "            out_channels:\n",
    "            mode: represent the streams coming in and out. e.g., ('2d', 'both'): one input stream (2d) and two output streams (2d and 3d)\n",
    "            FMU: determine the type of feature fusion if there are two input streams\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mode_in, self.mode_out = mode\n",
    "        self.FMU = FMU\n",
    "        norm_args = (self.norm_kwargs, self.norm_kwargs)\n",
    "        activation_args = (self.activation_kwargs, self.activation_kwargs)\n",
    "\n",
    "        if self.mode_out == '2d' or self.mode_out == 'both':\n",
    "            self.CB2d = CB3d(in_channels=in_channels, out_channels=out_channels,\n",
    "                             kSize=((1, 3, 3), (1, 3, 3)), stride=(1, 1), padding=(0, 1, 1),\n",
    "                             norm_args=norm_args, activation_args=activation_args)\n",
    "\n",
    "        if self.mode_out == '3d' or self.mode_out == 'both':\n",
    "            self.CB3d = CB3d(in_channels=in_channels, out_channels=out_channels,\n",
    "                             kSize=(3, 3), stride=(1, 1), padding=(1, 1, 1),\n",
    "                             norm_args=norm_args, activation_args=activation_args)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x2d, xskip2d, x3d, xskip3d = x\n",
    "\n",
    "        tarSize = xskip2d.shape[2:]\n",
    "        up2d = F.interpolate(x2d, size=tarSize, mode='trilinear', align_corners=False)\n",
    "        up3d = F.interpolate(x3d, size=tarSize, mode='trilinear', align_corners=False)\n",
    "\n",
    "        cat = torch.cat([FMU(xskip2d, xskip3d, self.FMU), FMU(up2d, up3d, self.FMU)], dim=1)\n",
    "\n",
    "        if self.mode_out == '2d':\n",
    "            return self.CB2d(cat)\n",
    "        elif self.mode_out == '3d':\n",
    "            return self.CB3d(cat)\n",
    "        elif self.mode_out == 'both':\n",
    "            return self.CB2d(cat), self.CB3d(cat)\n",
    "\n",
    "\n",
    "\n",
    "class MNet(BasicNet):\n",
    "    def __init__(self, in_channels, num_classes, kn=(32, 48, 64, 80, 96), ds=True, FMU='sub'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: channels of input\n",
    "            num_classes: output classes\n",
    "            kn: the number of kernels\n",
    "            ds: deep supervision\n",
    "            FMU: type of feature merging unit\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        channel_factor = {'sum': 1, 'sub': 1, 'cat': 2}\n",
    "        fct = channel_factor[FMU]\n",
    "\n",
    "        self.down11 = Down(in_channels, kn[0], ('/', 'both'), downsample=False)\n",
    "        self.down12 = Down(kn[0], kn[1], ('2d', 'both'))\n",
    "        self.down13 = Down(kn[1], kn[2], ('2d', 'both'))\n",
    "        self.down14 = Down(kn[2], kn[3], ('2d', 'both'))\n",
    "        self.bottleneck1 = Down(kn[3], kn[4], ('2d', '2d'))\n",
    "        self.up11 = Up(fct * (kn[3] + kn[4]), kn[3], ('both', '2d'), FMU)\n",
    "        self.up12 = Up(fct * (kn[2] + kn[3]), kn[2], ('both', '2d'), FMU)\n",
    "        self.up13 = Up(fct * (kn[1] + kn[2]), kn[1], ('both', '2d'), FMU)\n",
    "        self.up14 = Up(fct * (kn[0] + kn[1]), kn[0], ('both', 'both'), FMU)\n",
    "\n",
    "        self.down21 = Down(kn[0], kn[1], ('3d', 'both'))\n",
    "        self.down22 = Down(fct * kn[1], kn[2], ('both', 'both'), FMU)\n",
    "        self.down23 = Down(fct * kn[2], kn[3], ('both', 'both'), FMU)\n",
    "        self.bottleneck2 = Down(fct * kn[3], kn[4], ('both', 'both'), FMU)\n",
    "        self.up21 = Up(fct * (kn[3] + kn[4]), kn[3], ('both', 'both'), FMU)\n",
    "        self.up22 = Up(fct * (kn[2] + kn[3]), kn[2], ('both', 'both'), FMU)\n",
    "        self.up23 = Up(fct * (kn[1] + kn[2]), kn[1], ('both', '3d'), FMU)\n",
    "\n",
    "        self.down31 = Down(kn[1], kn[2], ('3d', 'both'))\n",
    "        self.down32 = Down(fct * kn[2], kn[3], ('both', 'both'), FMU)\n",
    "        self.bottleneck3 = Down(fct * kn[3], kn[4], ('both', 'both'), FMU)\n",
    "        self.up31 = Up(fct * (kn[3] + kn[4]), kn[3], ('both', 'both'), FMU)\n",
    "        self.up32 = Up(fct * (kn[2] + kn[3]), kn[2], ('both', '3d'), FMU)\n",
    "\n",
    "        self.down41 = Down(kn[2], kn[3], ('3d', 'both'), FMU)\n",
    "        self.bottleneck4 = Down(fct * kn[3], kn[4], ('both', 'both'), FMU)\n",
    "        self.up41 = Up(fct * (kn[3] + kn[4]), kn[3], ('both', '3d'), FMU)\n",
    "\n",
    "        self.bottleneck5 = Down(kn[3], kn[4], ('3d', '3d'))\n",
    "\n",
    "\n",
    "        self.outputs = nn.ModuleList(\n",
    "            [nn.Conv3d(c, num_classes, kernel_size=(1, 1, 1), stride=1, padding=0, bias=False)\n",
    "             for c in [kn[0], kn[1], kn[1], kn[2], kn[2], kn[3], kn[3]]]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        down11 = self.down11(x)\n",
    "        down12 = self.down12(down11[0])\n",
    "        down13 = self.down13(down12[0])\n",
    "        down14 = self.down14(down13[0])\n",
    "        bottleNeck1 = self.bottleneck1(down14[0])\n",
    "\n",
    "        down21 = self.down21(down11[1])\n",
    "        down22 = self.down22([down21[0], down12[1]])\n",
    "        down23 = self.down23([down22[0], down13[1]])\n",
    "        bottleNeck2 = self.bottleneck2([down23[0], down14[1]])\n",
    "\n",
    "        down31 = self.down31(down21[1])\n",
    "        down32 = self.down32([down31[0], down22[1]])\n",
    "        bottleNeck3 = self.bottleneck3([down32[0], down23[1]])\n",
    "\n",
    "        down41 = self.down41(down31[1])\n",
    "        bottleNeck4 = self.bottleneck4([down41[0], down32[1]])\n",
    "\n",
    "        bottleNeck5 = self.bottleneck5(down41[1])\n",
    "\n",
    "        up41 = self.up41([bottleNeck4[0], down41[0], bottleNeck5, down41[1]])\n",
    "\n",
    "        up31 = self.up31([bottleNeck3[0], down32[0], bottleNeck4[1], down32[1]])\n",
    "        up32 = self.up32([up31[0], down31[0], up41, down31[1]])\n",
    "\n",
    "        up21 = self.up21([bottleNeck2[0], down23[0], bottleNeck3[1], down23[1]])\n",
    "        up22 = self.up22([up21[0], down22[0], up31[1], down22[1]])\n",
    "        up23 = self.up23([up22[0], down21[0], up32, down21[1]])\n",
    "        \n",
    "        up11 = self.up11([bottleNeck1, down14[0], bottleNeck2[1], down14[1]])\n",
    "        up12 = self.up12([up11, down13[0], up21[1], down13[1]])\n",
    "        up13 = self.up13([up12, down12[0], up22[1], down12[1]])\n",
    "        up14 = self.up14([up13, down11[0], up23, down11[1]])\n",
    "\n",
    "\n",
    "        if self.ds:\n",
    "            features = [up14[0]+up14[1], up23, up13, up32, up12, up41,up11]\n",
    "            return [self.outputs[i](features[i]) for i in range(7)]\n",
    "        else:\n",
    "            return self.outputs[0](up14[0]+up14[1])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57aa1e76-f143-4726-a20d-0a3853047125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class KitsDataset(Dataset):\n",
    "    def __init__(self, img, seg):\n",
    "        '''\n",
    "        TODO: Store `seqs`. to `self.x` and `hfs` to `self.y`.\n",
    "\n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        Do NOT permute the data.\n",
    "        '''\n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "    \n",
    "        self.img = img\n",
    "        self.seg = seg\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        TODO: Return the number of samples (i.e. patients).\n",
    "        '''\n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "        return len(self.img)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        TODO: Generates one sample of data.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        '''\n",
    "        return self.img[index], self.seg[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e51c8855-9d51-4b72-86c1-856262228fd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torchmetrics\u001b[38;5;241m.\u001b[39mDice()\n\u001b[0;32m----> 2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01ae2b91-d7d5-41a4-b794-4958b2cfe297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Data/kits19/data/case_00010/imaging.nii.gz\n",
      "./Data/kits19/data/case_00011/imaging.nii.gz\n",
      "./Data/kits19/data/case_00012/imaging.nii.gz\n",
      "./Data/kits19/data/case_00013/imaging.nii.gz\n",
      "./Data/kits19/data/case_00014/imaging.nii.gz\n"
     ]
    }
   ],
   "source": [
    "im_list = []\n",
    "sg_list = []\n",
    "\n",
    "for i in len(t2_list):#range(len(t2_list)):\n",
    "    print(img_list[i])\n",
    "    temp_img = nib.load(img_list[i]).get_fdata()\n",
    "    temp_img = torch.from_numpy(scaler.fit_transform(temp_img.reshape(-1,temp_img.shape[-1])).reshape(temp_img.shape))\n",
    "    #print(temp_img.shape)\n",
    "    #print(temp_img)\n",
    "    temp_seg = nib.load(seg_list[i]).get_fdata()\n",
    "    temp_seg = torch.from_numpy(scaler.fit_transform(temp_seg.reshape(-1,temp_seg.shape[-1])).reshape(temp_seg.shape))\n",
    "    \n",
    "    im_list.append(temp_img)\n",
    "    sg_list.append(temp_seg)\n",
    "    '''\n",
    "    temp_t2 = nib.load(t2_list[i]).get_fdata()\n",
    "    temp_t2 = scaler.fit_transform(temp_t2.reshape(-1,temp_t2.shape[-1])).reshape(temp_t2.shape)\n",
    "    \n",
    "    temp_t1=nib.load(t1ce_list[i]).get_fdata()\n",
    "    temp_t1=scaler.fit_transform(temp_t1.reshape(-1, temp_t1.shape[-1])).reshape(temp_t1.shape)\n",
    "   \n",
    "    temp_flair=nib.load(flair_list[i]).get_fdata()\n",
    "    temp_flair=scaler.fit_transform(temp_flair.reshape(-1, temp_flair.shape[-1])).reshape(temp_flair.shape)\n",
    "        \n",
    "    temp_mask=nib.load(mask_list[i]).get_fdata()\n",
    "    temp_mask=temp_mask.astype(np.uint8)\n",
    "    temp_mask[temp_mask==4] = 3 \n",
    "    \n",
    "    t2_img_list.append(temp_t2)\n",
    "    t1_img_list.append(temp_t1)\n",
    "    flair_img_list.append(temp_flair)\n",
    "    mask_img_list.append(temp_mask)\n",
    "    '''\n",
    "kits_dataset = KitsDataset(im_list, sg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b024051e-aa47-456d-9436-8b3a9e6c5a5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kits_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> 2\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mkits_dataset\u001b[49m,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m mn \u001b[38;5;241m=\u001b[39m MNet(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, kn\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), ds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, FMU\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      4\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torchmetrics\u001b[38;5;241m.\u001b[39mDice()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kits_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(kits_dataset,batch_size=1)\n",
    "mn = MNet(1, 3, kn=(2, 2, 2, 2, 2), ds=False, FMU='sub').float()\n",
    "criterion = torchmetrics.Dice()\n",
    "optimizer = torch.optim.SGD(mn.parameters(), lr=0.001) \n",
    "assert isinstance(optimizer, torch.optim.SGD)\n",
    "\n",
    "\n",
    "mn.train()\n",
    "for (img,mask) in dataloader:  \n",
    "    optimizer.zero_grad()\n",
    "    output = mn(torch.unsqueeze(img.float(),dim=0))\n",
    "    pred = torch.unsqueeze(torch.sum(output[0],dim=0),dim=0) \n",
    "    pred[pred<0] = 0 \n",
    "    loss = criterion(pred,mask.int())\n",
    "    loss.backward()\n",
    "    optimizer.step() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
